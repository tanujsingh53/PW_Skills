{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1:  What is Ensemble Learning in machine learning? Explain the key idea behind it. **\n",
        "\n",
        "Ans: Ensemble Learning in machine learning is a technique where multiple models (often called \"learners\" or \"base models\") are combined to solve a particular problem‚Äîusually to improve prediction performance over a single model.\n",
        "\n",
        "Key Idea Behind Ensemble Learning:\n",
        "\n",
        "\"A group of weak models can come together to form a strong model.\"\n",
        "\n",
        "This is based on the principle that while individual models might make errors or have limitations, their combined predictions‚Äîif aggregated cleverly‚Äîcan cancel out individual weaknesses and produce more accurate, stable, and robust results.\n",
        "\n",
        "\n",
        "Q2: What is the difference between Bagging and Boosting? **bold text**\n",
        "\n",
        "Ans:\n",
        "Bagging and Boosting are both popular ensemble learning techniques, but they differ in their approach to combining models. Here's a breakdown of their key differences:\n",
        "\n",
        "Bagging (Bootstrap Aggregating):\n",
        "\n",
        "Parallel Processing: Base models are trained in parallel, independently of each other.\n",
        "Voting/Averaging: The final prediction is typically made by averaging (for regression) or voting (for classification) the predictions of individual models.\n",
        "Reduced Variance: Bagging primarily aims to reduce variance by training models on different subsets of the data (created through bootstrapping). It's less effective at reducing bias.\n",
        "Example: Random Forests, where multiple decision trees are trained on bootstrapped samples of the data.\n",
        "Boosting:\n",
        "\n",
        "Sequential Processing: Base models are trained sequentially, where each new model attempts to correct the errors of the previous ones.\n",
        "Weighted Voting: The final prediction is a weighted combination of the predictions of individual models. Models that perform better on the training data are given higher weights.\n",
        "Reduced Bias: Boosting primarily aims to reduce bias by focusing on misclassified instances in each iteration. It's more prone to overfitting if not carefully tuned.\n",
        "\n",
        "Examples: AdaBoost, Gradient Boosting Machines (GBM), and XGBoost.\n",
        "In summary, Bagging builds multiple independent models and averages their predictions to reduce variance, while Boosting builds models sequentially, with each new model trying to improve upon the previous one, primarily to reduce bias.\n",
        "\n",
        "\n",
        "**Q3:  What is bootstrap sampling and what role does it play in Bagging methods like Random Forest? **\n",
        "\n",
        "Ans:\n",
        "Bootstrap sampling is a technique where we randomly select samples with replacement from the original dataset to create new training datasets.\n",
        "\n",
        "In Bagging (e.g., Random Forest):\n",
        "\n",
        "Each model is trained on a different bootstrap sample.\n",
        "\n",
        "This introduces diversity among models, helping reduce overfitting and variance.\n",
        "\n",
        "Final predictions are made by aggregating (e.g., majority vote or averaging) the results of all models.\n",
        "\n",
        "Key Point: Bootstrap sampling creates varied data for each model, making the ensemble stronger and more stable.\n",
        "\n",
        "\n",
        "**Q4:  What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models? **\n",
        "\n",
        "Ans: Out-of-Bag (OOB) samples are the data points not included in a model‚Äôs bootstrap sample during training in Bagging methods like Random Forest.\n",
        "\n",
        "üîç OOB Score:\n",
        "\n",
        "Since each tree sees only part of the data, OOB samples can be used as a validation set.\n",
        "\n",
        "The OOB score is the average prediction accuracy on these unused samples.\n",
        "\n",
        "It provides a built-in estimate of model performance, without needing a separate validation set.\n",
        "\n",
        "Key Point: OOB score is a quick and reliable way to evaluate ensemble models like Random Forest using the data already available.\n",
        "\n",
        "\n",
        "**Q5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.**\n",
        "\n",
        "Ans: In a single Decision Tree, feature importance is based on how much each feature reduces impurity (like Gini impurity or entropy) when used for splitting. It's a local view based on that specific tree.\n",
        "\n",
        "In a Random Forest, feature importance is an aggregate measure across all the trees in the forest. It's typically calculated by averaging the importance of each feature across all trees. This provides a more robust and less-biased estimate of a feature's overall relevance because it considers the feature's impact across different subsets of data and features used in each tree.\n",
        "\n",
        "\n",
        "**Q6:  Write a Python program to:\n",
        "\n",
        "‚óè Load the Breast Cancer dataset using\n",
        "\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "\n",
        "‚óè Train a Random Forest Classifier\n",
        "\n",
        "‚óè Print the top 5 most important features based on feature importance scores.**\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "9tF4wv6ThWg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "# We are using a fixed random_state for reproducibility\n",
        "# n_estimators is set to 100, which is a common default.\n",
        "# You can adjust these hyperparameters as needed.\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = pd.Series(rf_classifier.feature_importances_, index=X.columns)\n",
        "\n",
        "# Print the top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_importances.nlargest(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaZQHWYhjsVX",
        "outputId": "3c3d1308-fe03-46a8-8081-02dd10b27e2c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7:  Write a Python program to:\n",
        "\n",
        "‚óè Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "\n",
        "‚óè Evaluate its accuracy and compare with a single Decision Tree **"
      ],
      "metadata": {
        "id": "zSc9iDh4jymu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree Classifier\n",
        "single_tree = DecisionTreeClassifier(random_state=42)\n",
        "single_tree.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the single Decision Tree\n",
        "y_pred_single = single_tree.predict(X_test)\n",
        "accuracy_single = accuracy_score(y_test, y_pred_single)\n",
        "print(f\"Accuracy of a single Decision Tree: {accuracy_single:.4f}\")\n",
        "\n",
        "# Train a Bagging Classifier using Decision Trees\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(random_state=42),\n",
        "                                n_estimators=10, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the Bagging Classifier\n",
        "y_pred_bagging = bagging_clf.predict(X_test)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "print(f\"Accuracy of Bagging Classifier: {accuracy_bagging:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLXm16MPj74v",
        "outputId": "b0194778-b4a5-4314-85cb-234684bae10a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of a single Decision Tree: 1.0000\n",
            "Accuracy of Bagging Classifier: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8: Write a Python program to:\n",
        "\n",
        "‚óè Train a Random Forest Classifier\n",
        "\n",
        "‚óè Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "\n",
        "‚óè Print the best parameters and final accuracy **"
      ],
      "metadata": {
        "id": "KVb76TBhkeSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid to tune\n",
        "param_grid = {\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'n_estimators': [50, 100, 200]\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV to find the best hyperparameters\n",
        "grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Evaluate the model with the best hyperparameters on the test set\n",
        "best_rf_clf = grid_search.best_estimator_\n",
        "y_pred_rf = best_rf_clf.predict(X_test)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(f\"Accuracy of the tuned Random Forest Classifier: {accuracy_rf:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm-jUqKzksbe",
        "outputId": "4a4b95ea-c361-4e2d-d8ab-805762face4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Accuracy of the tuned Random Forest Classifier: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9:  Write a Python program to: ‚óè Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset ‚óè Compare their Mean Squared Errors (MSE)**"
      ],
      "metadata": {
        "id": "6hxTbZLFlErX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Bagging Regressor using Decision Tree Regressor as base estimator\n",
        "bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(random_state=42),\n",
        "                               n_estimators=10, random_state=42)\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the Bagging Regressor\n",
        "y_pred_bagging = bagging_reg.predict(X_test)\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "print(f\"Mean Squared Error of Bagging Regressor: {mse_bagging:.4f}\")\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the Random Forest Regressor\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "print(f\"Mean Squared Error of Random Forest Regressor: {mse_rf:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXuYbI2OlZPW",
        "outputId": "518d1a5d-e380-4c69-f915-2dd49d03c8f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error of Bagging Regressor: 0.2862\n",
            "Mean Squared Error of Random Forest Regressor: 0.2565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. You decide to use ensemble techniques to increase model performance. Explain your step-by-step approach to:\n",
        "\n",
        "‚óè Choose between Bagging or Boosting\n",
        "\n",
        "‚óè Handle overfitting\n",
        "\n",
        "‚óè Select base models\n",
        "\n",
        "‚óè Evaluate performance using cross-validation\n",
        "\n",
        "‚óè Justify how ensemble learning improves decision-making in this real-world context. **"
      ],
      "metadata": {
        "id": "Jry3Q6iNlck_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Choose Between Bagging or Boosting\n",
        "\n",
        "Start with Boosting (e.g., XGBoost/LightGBM) since it typically performs better on imbalanced classification tasks like loan default by focusing on hard-to-predict cases.\n",
        "\n",
        "Use Bagging (e.g., Random Forest) if the data is noisy or high-variance, as it helps reduce variance through model averaging.\n",
        "\n",
        "2. Handle Overfitting\n",
        "\n",
        "Boosting: Limit tree depth, increase learning rate, apply early stopping.\n",
        "\n",
        "Bagging: Use more trees, set max features per split, apply pruning if needed.\n",
        "\n",
        "Regularization techniques (L1/L2) and cross-validation are applied in both.\n",
        "\n",
        "3. Select Base Models\n",
        "\n",
        "Tree-based models (e.g., decision trees) are common due to their robustness and interpretability.\n",
        "\n",
        "For stacking: use diverse models (e.g., logistic regression, SVM, trees) to capture different patterns.\n",
        "\n",
        "4. Evaluate Performance Using Cross-Validation\n",
        "\n",
        "Use Stratified K-Fold CV to maintain class distribution in each fold.\n",
        "\n",
        "Evaluate using AUC-ROC, precision, recall, F1-score, focusing on minimizing false negatives (missed defaults).\n",
        "\n",
        "5. Justify Ensemble Learning in Context\n",
        "\n",
        "Improves accuracy and robustness by combining multiple models.\n",
        "\n",
        "Reduces model bias or variance, depending on technique.\n",
        "\n",
        "Leads to more reliable risk assessments, improving loan approval decisions and reducing financial loss from defaults."
      ],
      "metadata": {
        "id": "vsjwcXQJmXlX"
      }
    }
  ]
}