{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "839a4597"
      },
      "source": [
        "Q1:  What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "Ans:  K-Nearest Neighbors (KNN) is a simple, non-parametric machine learning algorithm used for both classification and regression.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "*   **Classification:** To classify a new data point, KNN considers the 'k' nearest data points (neighbors) in the training set. The new point is assigned the class that is most common among its 'k' neighbors.\n",
        "*   **Regression:** For regression, KNN calculates the average (or weighted average) of the target values of the 'k' nearest neighbors to predict the value for the new data point.\n",
        "\n",
        "The 'k' value is a crucial parameter that needs to be chosen carefully, as it impacts the model's performance and complexity.\n",
        "\n",
        "Q2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        "Ans:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49040c77"
      },
      "source": [
        "Q1:  What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "Ans:  K-Nearest Neighbors (KNN) is a simple, non-parametric machine learning algorithm used for both classification and regression.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "* **Classification:** To classify a new data point, KNN considers the 'k' nearest data points (neighbors) in the training set. The new point is assigned the class that is most common among its 'k' neighbors.\n",
        "* **Regression:** For regression, KNN calculates the average (or weighted average) of the target values of the 'k' nearest neighbors to predict the value for the new data point.\n",
        "\n",
        "The 'k' value is a crucial parameter that needs to be chosen carefully, as it impacts the model's performance and complexity.\n",
        "\n",
        "Q2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        "Ans: The **Curse of Dimensionality** refers to the challenges that arise when working with high-dimensional data (data with many features). In high-dimensional spaces, data points become increasingly sparse, meaning the distance between any two points becomes large.\n",
        "\n",
        "**How it affects KNN:**\n",
        "\n",
        "* **Increased distance:** In high dimensions, the concept of \"nearest\" neighbors becomes less meaningful as all points tend to be far away from each other.\n",
        "* **Computational cost:** Calculating distances between points in high dimensions is computationally expensive.\n",
        "* **Overfitting:** With sparse data, KNN can easily overfit to the training data, leading to poor performance on new data.\n",
        "\n",
        "In essence, as the number of dimensions increases, the performance of KNN tends to degrade due to the sparsity of data and the increased difficulty in finding truly \"nearest\" neighbors.\n",
        "\n",
        "Q3: : What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "\n",
        "Ans:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8977c38"
      },
      "source": [
        "Q1:  What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "Ans:  K-Nearest Neighbors (KNN) is a simple, non-parametric machine learning algorithm used for both classification and regression.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "* **Classification:** To classify a new data point, KNN considers the 'k' nearest data points (neighbors) in the training set. The new point is assigned the class that is most common among its 'k' neighbors.\n",
        "* **Regression:** For regression, KNN calculates the average (or weighted average) of the target values of the 'k' nearest neighbors to predict the value for the new data point.\n",
        "\n",
        "The 'k' value is a crucial parameter that needs to be chosen carefully, as it impacts the model's performance and complexity.\n",
        "\n",
        "Q2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        "Ans: The **Curse of Dimensionality** refers to the challenges that arise when working with high-dimensional data (data with many features). In high-dimensional spaces, data points become increasingly sparse, meaning the distance between any two points becomes large.\n",
        "\n",
        "**How it affects KNN:**\n",
        "\n",
        "* **Increased distance:** In high dimensions, the concept of \"nearest\" neighbors becomes less meaningful as all points tend to be far away from each other.\n",
        "* **Computational cost:** Calculating distances between points in high dimensions is computationally expensive.\n",
        "* **Overfitting:** With sparse data, KNN can easily overfit to the training data, leading to poor performance on new data.\n",
        "\n",
        "In essence, as the number of dimensions increases, the performance of KNN tends to degrade due to the sparsity of data and the increased difficulty in finding truly \"nearest\" neighbors.\n",
        "\n",
        "Q3: : What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "\n",
        "Ans: **Principal Component Analysis (PCA)** is a dimensionality reduction technique that transforms the original features into a new set of uncorrelated features called principal components. These components capture most of the variance in the data, allowing you to represent the data in a lower-dimensional space while retaining important information.\n",
        "\n",
        "**Difference from feature selection:**\n",
        "\n",
        "*   **PCA:** Creates new features (principal components) that are linear combinations of the original features. It aims to capture the most variance in the data.\n",
        "*   **Feature Selection:** Selects a subset of the original features based on criteria like relevance or importance. It discards the less important original features.\n",
        "\n",
        "In simple terms, PCA creates entirely new features, while feature selection chooses from the existing ones.\n",
        "\n",
        "\n",
        "Q4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "Ans:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "556412aa"
      },
      "source": [
        "Q1:  What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "Ans:  K-Nearest Neighbors (KNN) is a simple, non-parametric machine learning algorithm used for both classification and regression.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "* **Classification:** To classify a new data point, KNN considers the 'k' nearest data points (neighbors) in the training set. The new point is assigned the class that is most common among its 'k' neighbors.\n",
        "* **Regression:** For regression, KNN calculates the average (or weighted average) of the target values of the 'k' nearest neighbors to predict the value for the new data point.\n",
        "\n",
        "The 'k' value is a crucial parameter that needs to be chosen carefully, as it impacts the model's performance and complexity.\n",
        "\n",
        "Q2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        "Ans: The **Curse of Dimensionality** refers to the challenges that arise when working with high-dimensional data (data with many features). In high-dimensional spaces, data points become increasingly sparse, meaning the distance between any two points becomes large.\n",
        "\n",
        "**How it affects KNN:**\n",
        "\n",
        "* **Increased distance:** In high dimensions, the concept of \"nearest\" neighbors becomes less meaningful as all points tend to be far away from each other.\n",
        "* **Computational cost:** Calculating distances between points in high dimensions is computationally expensive.\n",
        "* **Overfitting:** With sparse data, KNN can easily overfit to the training data, leading to poor performance on new data.\n",
        "\n",
        "In essence, as the number of dimensions increases, the performance of KNN tends to degrade due to the sparsity of data and the increased difficulty in finding truly \"nearest\" neighbors.\n",
        "\n",
        "Q3: : What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "\n",
        "Ans: **Principal Component Analysis (PCA)** is a dimensionality reduction technique that transforms the original features into a new set of uncorrelated features called principal components. These components capture most of the variance in the data, allowing you to represent the data in a lower-dimensional space while retaining important information.\n",
        "\n",
        "**Difference from feature selection:**\n",
        "\n",
        "*   **PCA:** Creates new features (principal components) that are linear combinations of the original features. It aims to capture the most variance in the data.\n",
        "*   **Feature Selection:** Selects a subset of the original features based on criteria like relevance or importance. It discards the less important original features.\n",
        "\n",
        "In simple terms, PCA creates entirely new features, while feature selection chooses from the existing ones.\n",
        "\n",
        "Q4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "Ans: **Eigenvectors** represent the directions (axes) in the data that explain the most variance. **Eigenvalues** are the magnitudes associated with these eigenvectors, indicating the amount of variance explained by each eigenvector.\n",
        "\n",
        "**Importance in PCA:**\n",
        "\n",
        "*   Eigenvectors define the principal components, which are the new axes in the lower-dimensional space.\n",
        "*   Eigenvalues help determine which principal components are most important to keep, as components with larger eigenvalues capture more of the data's variance.\n",
        "\n",
        "By selecting the eigenvectors with the largest eigenvalues, PCA can reduce the dimensionality of the data while retaining the most significant information.\n",
        "\n",
        "\n",
        "Q5: How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "\n",
        "Ans:  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UBhkicg1WOqb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aedc017"
      },
      "source": [
        "Q1:  What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "Ans:  K-Nearest Neighbors (KNN) is a simple, non-parametric machine learning algorithm used for both classification and regression.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "* **Classification:** To classify a new data point, KNN considers the 'k' nearest data points (neighbors) in the training set. The new point is assigned the class that is most common among its 'k' neighbors.\n",
        "* **Regression:** For regression, KNN calculates the average (or weighted average) of the target values of the 'k' nearest neighbors to predict the value for the new data point.\n",
        "\n",
        "The 'k' value is a crucial parameter that needs to be chosen carefully, as it impacts the model's performance and complexity.\n",
        "\n",
        "Q2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        "Ans: The **Curse of Dimensionality** refers to the challenges that arise when working with high-dimensional data (data with many features). In high-dimensional spaces, data points become increasingly sparse, meaning the distance between any two points becomes large.\n",
        "\n",
        "**How it affects KNN:**\n",
        "\n",
        "* **Increased distance:** In high dimensions, the concept of \"nearest\" neighbors becomes less meaningful as all points tend to be far away from each other.\n",
        "* **Computational cost:** Calculating distances between points in high dimensions is computationally expensive.\n",
        "* **Overfitting:** With sparse data, KNN can easily overfit to the training data, leading to poor performance on new data.\n",
        "\n",
        "In essence, as the number of dimensions increases, the performance of KNN tends to degrade due to the sparsity of data and the increased difficulty in finding truly \"nearest\" neighbors.\n",
        "\n",
        "Q3: : What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "\n",
        "Ans: **Principal Component Analysis (PCA)** is a dimensionality reduction technique that transforms the original features into a new set of uncorrelated features called principal components. These components capture most of the variance in the data, allowing you to represent the data in a lower-dimensional space while retaining important information.\n",
        "\n",
        "**Difference from feature selection:**\n",
        "\n",
        "*   **PCA:** Creates new features (principal components) that are linear combinations of the original features. It aims to capture the most variance in the data.\n",
        "*   **Feature Selection:** Selects a subset of the original features based on criteria like relevance or importance. It discards the less important original features.\n",
        "\n",
        "In simple terms, PCA creates entirely new features, while feature selection chooses from the existing ones.\n",
        "\n",
        "Q4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "Ans: **Eigenvectors** represent the directions (axes) in the data that explain the most variance. **Eigenvalues** are the magnitudes associated with these eigenvectors, indicating the amount of variance explained by each eigenvector.\n",
        "\n",
        "**Importance in PCA:**\n",
        "\n",
        "*   Eigenvectors define the principal components, which are the new axes in the lower-dimensional space.\n",
        "*   Eigenvalues help determine which principal components are most important to keep, as components with larger eigenvalues capture more of the data's variance.\n",
        "\n",
        "By selecting the eigenvectors with the largest eigenvalues, PCA can reduce the dimensionality of the data while retaining the most significant information.\n",
        "\n",
        "Q5: How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "Ans: KNN and PCA can be used together in a pipeline to improve the performance of KNN, especially when dealing with high-dimensional data.\n",
        "\n",
        "Here's how they complement each other:\n",
        "\n",
        "1.  **PCA for dimensionality reduction:** PCA can be applied before KNN to reduce the number of features in the dataset. This helps mitigate the effects of the Curse of Dimensionality on KNN by reducing computational cost, decreasing the risk of overfitting, and making the concept of \"nearest\" neighbors more meaningful.\n",
        "2.  **KNN on reduced dimensions:** Once the data is transformed into a lower-dimensional space using PCA, KNN can be applied to the principal components. This allows KNN to operate on a more manageable and informative representation of the data, potentially leading to improved accuracy and efficiency.\n",
        "\n",
        "In essence, PCA acts as a preprocessing step to prepare high-dimensional data for KNN, making KNN more effective and less susceptible to the challenges of high-dimensional spaces."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6:  Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases."
      ],
      "metadata": {
        "id": "LLQ1d4ImXrRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train KNN without feature scaling\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# Train KNN with feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy with scaling: {accuracy_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt30FUyUWdwe",
        "outputId": "f8f86d89-58c9-491f-f1b2-1be3b0c43621"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7407\n",
            "Accuracy with scaling: 0.9630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7: Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component.\n"
      ],
      "metadata": {
        "id": "57rniLZ_X-A7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# Train a PCA model\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "# Print the explained variance ratio\n",
        "print(\"Explained variance ratio of each principal component:\")\n",
        "print(pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_aeqthmYQQ0",
        "outputId": "16a7d2eb-1250-4078-db35-4e1d4fd0feef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio of each principal component:\n",
            "[9.98091230e-01 1.73591562e-03 9.49589576e-05 5.02173562e-05\n",
            " 1.23636847e-05 8.46213034e-06 2.80681456e-06 1.52308053e-06\n",
            " 1.12783044e-06 7.21415811e-07 3.78060267e-07 2.12013755e-07\n",
            " 8.25392788e-08]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset.\n"
      ],
      "metadata": {
        "id": "8slQajZEYa1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train KNN without feature scaling\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# Train KNN with feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Train KNN on PCA-transformed data (2 components)\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled) # Apply PCA after scaling\n",
        "X_test_pca = pca.transform(X_test_scaled) # Apply PCA after scaling\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy with scaling: {accuracy_scaled:.4f}\")\n",
        "print(f\"Accuracy with PCA (2 components): {accuracy_pca:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XjcesfnYqYf",
        "outputId": "2f2b9db5-8c77-4284-e7dd-081635caa1bf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7407\n",
            "Accuracy with scaling: 0.9630\n",
            "Accuracy with PCA (2 components): 0.9815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9: : Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results."
      ],
      "metadata": {
        "id": "G4YxosXRZBmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# 2. Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Train and evaluate KNN with Euclidean distance\n",
        "knn_euclidean = KNeighborsClassifier(metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "print(f\"Accuracy with Euclidean distance: {accuracy_euclidean}\")\n",
        "\n",
        "# 5. Train and evaluate KNN with Manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "print(f\"Accuracy with Manhattan distance: {accuracy_manhattan}\")\n",
        "\n",
        "# 6. Compare results\n",
        "print(\"\\nComparison of results:\")\n",
        "print(f\"Accuracy with Euclidean distance: {accuracy_euclidean}\")\n",
        "print(f\"Accuracy with Manhattan distance: {accuracy_manhattan}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgIx3VqYaHR9",
        "outputId": "ffa7cdae-951a-48a7-fe0e-a85052c06ec6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean distance: 0.9629629629629629\n",
            "Accuracy with Manhattan distance: 0.9629629629629629\n",
            "\n",
            "Comparison of results:\n",
            "Accuracy with Euclidean distance: 0.9629629629629629\n",
            "Accuracy with Manhattan distance: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10:  You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "\n",
        "● Use PCA to reduce dimensionality\n",
        "\n",
        "● Decide how many components to keep\n",
        "\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "\n",
        "● Evaluate the model\n",
        "\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "\n",
        "\n",
        "\n",
        "Ans:\n",
        "\n",
        "1. PCA for Dimensionality Reduction – Standardize data → apply PCA to compress thousands of gene features into fewer principal components while retaining key biological variance.\n",
        "\n",
        "2. Choosing Components – Keep enough PCs to explain ~95% variance (or use elbow method).\n",
        "\n",
        "3. KNN Classification – Train KNN on the reduced dataset (distance works better in lower dimensions).\n",
        "\n",
        "4. Evaluation – Use cross-validation, accuracy, precision, recall, F1, and ROC-AUC to ensure reliable results.\n",
        "\n",
        "5. Justification to Stakeholders – This approach prevents overfitting, improves computational efficiency, and is widely used in biomedical data analysis for robust, generalizable cancer classification."
      ],
      "metadata": {
        "id": "hN74ogofbejI"
      }
    }
  ]
}